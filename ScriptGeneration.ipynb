{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a8f6c5-87da-4e2b-b36f-5031e30bc626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "# helper function from Udacity. This is the course I made this project for\n",
    "# anytime a helper function appears its a function provided by Udacity\n",
    "import helper\n",
    "data_dir = './data/Seinfeld_Scripts.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25847b-c66f-4256-a7f7-772c222fc5c1",
   "metadata": {},
   "source": [
    "### Understanding the data\n",
    "Printing some data statistics to understand it better and printing some paragraphs as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a090ddc-b5b9-4c66-8e06-abd5b4e9a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_line_range = (0, 10)\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('the lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40ad805-effa-4f7f-8451-ca065b40bd1a",
   "metadata": {},
   "source": [
    "### Creating a lookup table\n",
    "The lookup table assigns each word a number so it can be an input to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a741c-baa3-4167-a37a-3e8f4d970e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    # creating a lookup table\n",
    "    words = set(text)\n",
    "    int_to_vocab = {i:word for i,word in enumerate(words)}\n",
    "    vocab_to_int = {k:v for v,k in int_to_vocab.items()}\n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52126bf-dbf1-4104-9197-ac8302ced261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def token_lookup():\n",
    "    # tokens for punctuation\n",
    "    punct_tokens = {\"!\":\"||Exclamation||\", \".\":\"||Period||\", \",\":\"||comma||\", punctuation[1]:\"||Quotation||\",\n",
    "               \";\":\"||Semicolon||\", \"?\":\"||Question||\", \"(\":\"||Left_Parentheses||\", \")\":\"||Right_Parentheses||\",\n",
    "               \"-\":\"||Dash||\", \"\\n\":\"||Return||\"}\n",
    "    return punct_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e04415-4331-4d6d-8c9d-083b28c78972",
   "metadata": {},
   "source": [
    "### Checking for gpu availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d83fba3-f04d-4ddf-aadf-26230a7c52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print(\"GPU BABYYYYY!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f829dc2-3eb4-4a81-b193-2172fd53cbca",
   "metadata": {},
   "source": [
    "### Preprocessing data\n",
    "The helper function was provided by Udacity and it helped me preprocess the data using the functions I made above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56298e-8a5f-43b9-a864-447f3ec2687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956edc30-be70-440a-90f7-a64430bb9c85",
   "metadata": {},
   "source": [
    "### Batching data\n",
    "creates a function to seperate all the text into sequences where a certain number of words\n",
    "are put into a features list and the subsequent word is saved as the target. This is then\n",
    "converted to data using TensorDataset and that is put into a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a944ef9f-9c04-4f7a-b145-00fc9a9a9226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    n_batches = len(words)//(sequence_length)\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*sequence_length]\n",
    "    #words = np.array(words)\n",
    "    #words = words.reshape(n_batches, sequence_length)\n",
    "    #creates lists\n",
    "    x, y= [], []\n",
    "    for idx in range(0, len(words), sequence_length):\n",
    "        #seperates the batch into a section of the words from idx:idx+sequence length\n",
    "        batch = words[idx:idx+sequence_length]\n",
    "        #extracts the features and targets\n",
    "        features = batch[:sequence_length-1]\n",
    "        target = batch[sequence_length-1]\n",
    "        #adds to overall list\n",
    "        x.append(features)\n",
    "        y.append(target)\n",
    "        \n",
    "    # Expected tensor for argument #1 'indices'to have scalar type Long; \n",
    "    # but got CUDAFloatTensor instead (while checking arguments for embedding)\n",
    "    x = torch.from_numpy(np.array(x))\n",
    "    y = torch.from_numpy(np.array(y))\n",
    "    #converts to a dataloader\n",
    "    data = TensorDataset(x, y)\n",
    "    dataloader = DataLoader(data, batch_size = batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6134fa6-7831-486d-be72-9c1f761062a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataloader\n",
    "\n",
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab32bc-bbec-460b-a2a5-036f7fdc8105",
   "metadata": {},
   "source": [
    "### Defining the RNN perameters\n",
    "LSTM cells are used instead of RNN cells because it is better at handling the vanishing gradient problem and they can be trained to remember more important words and forget less important ones to make more accurate predictions. LSTM was used over GRU because LSTM cells have 3 gates (input, output, and forget) while GRU's have two (reset and update). This makes GRU cells better for smaller datasets and LSTM cells better for larger data sets, such as the one we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f33d55-eadd-40f4-8a82-7574aec21d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        super(RNN, self).__init__()\n",
    "        # TODO: Implement function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # set class variables\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # define model layers\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim*4, \n",
    "                            n_layers, dropout=dropout, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim*4, hidden_dim*3.5,\n",
    "                            n_layers, dropout=dropout, batch_dirst=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim*3.5, hidden_dim*3)\n",
    "        self.fc2 = nn.Linear(hidden_dim*3, hidden_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, nn_input, hidden):\n",
    "        batch_size = nn_input.size(0)\n",
    "\n",
    "        # embedding and lstm layer\n",
    "        x = self.embed(nn_input)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # reshape so lstm output can fit into fully connected layer\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully connected\n",
    "        fc_in = self.dropout(lstm_out)\n",
    "        fc_out = self.fc(fc_in)\n",
    "        \n",
    "        # adjust so you have the batch_size, layers, and output size\n",
    "        output = fc_out.view(batch_size, -1, self.output_size)\n",
    "        \n",
    "        # activation function\n",
    "        #output = self.sigmoid(output_in)\n",
    "        \n",
    "        # pull the final layer of outputs from lstm layers\n",
    "        out = output[:,-1]\n",
    "        \n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Implement function\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                 weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd946759-0bfa-4ccd-b15c-9bde1abce489",
   "metadata": {},
   "source": [
    "### Creates training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f067c50-b74a-41aa-8743-e941b86cdeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    # move data to GPU, if available\n",
    "    target, inp = target.cuda(), inp.cuda()\n",
    "    # clears gradient so it can backprop again\n",
    "    rnn.zero_grad()\n",
    "    # runs through forward prop\n",
    "    output, hidden = rnn(inp, hidden)\n",
    "    # calculates error function\n",
    "    loss = criterion(output, target)\n",
    "    # backpropogates\n",
    "    loss.backward()\n",
    "    # converts from tensor to float\n",
    "    #loss = float(loss.cpu().detach().numpy())\n",
    "    # helps prevent the exploding gradient problem\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "    # takes step\n",
    "    optimizer.step()\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f689bdf-ce35-4541-888c-c5ba13ba5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    previous_loss = np.inf\n",
    "    \n",
    "    rnn.train()\n",
    "    #print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)     \n",
    "            loss_initial = loss\n",
    "            \n",
    "            if previous_loss < loss_initial:\n",
    "                # training the model\n",
    "                torch.save(rnn.state_dict(), 'model.pt')\n",
    "            \n",
    "            previous_loss = loss\n",
    "            \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                print('Model Trained and Saved')\n",
    "                batch_losses = []\n",
    "                \n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa0622f-40f1-41ec-91e3-167d07beaa2e",
   "metadata": {},
   "source": [
    "### Batch parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13189d64-00c4-4cb3-a947-098d40ebbcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "# Sequence Length\n",
    "sequence_length = 10  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 120\n",
    "in_text = int_text[:390]\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bcf65b-af39-4488-9ea8-df6aac7d88f7",
   "metadata": {},
   "source": [
    "### Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6906a98-e283-437d-bfaf-aebf80499564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 8\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(int_to_vocab)\n",
    "# Output size\n",
    "output_size = vocab_size\n",
    "# Embedding Dimension\n",
    "embedding_dim = 500\n",
    "# Hidden Dimension\n",
    "hidden_dim = 100\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41814d-95ca-49cb-8934-e8ae3beba7a1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a44a07-972d-4ae4-b6a8-8a92b952d5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "\n",
    "# this is to continue training the previously saved model\n",
    "rnn.load_state_dict(torch.load('model.pt'), strict=True)\n",
    "\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "# saving the trained model\n",
    "helper.save_model('./save/trained_rnn', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556ef94-9975-43e8-84a0-a0f2ad02ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "trained_rnn = helper.load_model('./save/trained_rnn') ### helper code provided by Udacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61087e-4262-40bb-a8e8-5463b5bf2ebe",
   "metadata": {},
   "source": [
    "### Generating text using the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291dc72e-77c8-4ab1-a5ed-9892c6a08771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 1\n",
    "        _, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[top_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006483f2-707e-4f0a-980a-eb464cf73754",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_length = 400 # modify the length to your preference\n",
    "prime_word = 'jerry' # name for starting the script\n",
    "\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
