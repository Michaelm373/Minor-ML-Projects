{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e112b1-0e7b-4a4a-abda-2f46132092d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip summer2winter_yosemite.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70042e0b-c454-4855-806f-1d4cd6aea025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# visualizing data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc4989-5593-42d0-b800-d85d8000c772",
   "metadata": {},
   "source": [
    "### Data setup\n",
    "Creates data_loaders for the train and test data. This is then used to create the summer and winter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1351c023-36f0-4aa3-8075-3f3e22fd9f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(image_type, image_dir='summer2winter_yosemite', \n",
    "                    image_size=128, batch_size=16, num_workers=0):    \n",
    "    # resize and normalize the images\n",
    "    transform = transforms.Compose([transforms.Resize(image_size), # resize to 128x128\n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "    # get training and test directories\n",
    "    image_path = './' + image_dir\n",
    "    train_path = os.path.join(image_path, image_type)\n",
    "    test_path = os.path.join(image_path, 'test_{}'.format(image_type))\n",
    "\n",
    "    # define datasets using ImageFolder\n",
    "    train_dataset = datasets.ImageFolder(train_path, transform)\n",
    "    test_dataset = datasets.ImageFolder(test_path, transform)\n",
    "\n",
    "    # create and return DataLoaders\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac7e9e-e0fa-42fa-bc15-fa36e2cd00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test dataloaders for images from the two domains X and Y\n",
    "# image_type = directory names for our data\n",
    "dataloader_X, test_dataloader_X = get_data_loader(image_type='summer')\n",
    "dataloader_Y, test_dataloader_Y = get_data_loader(image_type='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c823b603-dce2-4416-abec-af121cb1a8c8",
   "metadata": {},
   "source": [
    "### Displays some images to make sure the data is batched properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e19b5-0491-4151-9ae9-d66f0c1705ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "\n",
    "# get some images from X\n",
    "dataiter = iter(dataloader_X)\n",
    "images, _ = dataiter.next()\n",
    "\n",
    "# show images\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165ef14-5e0a-46bc-9fc3-68c9b2db6c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some images from Y\n",
    "dataiter = iter(dataloader_Y)\n",
    "images, _ = dataiter.next()\n",
    "\n",
    "# show images\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137bebb4-39f8-48c0-a321-2a54c1d28c78",
   "metadata": {},
   "source": [
    "### Scaling function\n",
    "GANs have been shown to perform best with a tanh function at the end of the generator so the data has to be scaled accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ca7ad-b20c-4698-8d96-a5ec7e97a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x, feature_range=(-1, 1)):\n",
    "    # scale from 0-1 to feature_range\n",
    "    min, max = feature_range\n",
    "    x = x * (max - min) + min\n",
    "    return x\n",
    "scaled_img = scale(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc29a54-33eb-4ebc-b73e-e784801cfcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = images[0]\n",
    "\n",
    "print('Min: ', img.min())\n",
    "print('Max: ', img.max())\n",
    "\n",
    "print('='*18)\n",
    "\n",
    "print('Scaled min: ', scaled_img.min())\n",
    "print('Scaled max: ', scaled_img.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ba908c-9edc-4b8c-b37f-d61a63c796a5",
   "metadata": {},
   "source": [
    "### Creates discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d2e42f-9392-4b18-86d0-033f9a1edaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# discriminator function\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Define all convolutional layers\n",
    "        # Should accept an RGB image as input and output a single value\n",
    "        self.conv1 = nn.Conv2d(3, conv_dim, 4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(conv_dim, conv_dim*2, 4, stride=2, padding=1)\n",
    "        self.norm1 = nn.BatchNorm2d(conv_dim*2)\n",
    "        self.conv3 = nn.Conv2d(conv_dim*2, conv_dim*4, 4, stride=2, padding=1)\n",
    "        self.norm2 = nn.BatchNorm2d(conv_dim*4)\n",
    "        self.conv4 = nn.Conv2d(conv_dim*4, conv_dim*8, 4, stride=2, padding=1)\n",
    "        self.norm3 = nn.BatchNorm2d(conv_dim*8)\n",
    "        self.conv_final = nn.Conv2d(conv_dim*8, 1, 4, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # define feedforward behavior\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.norm1(self.conv2(x)))\n",
    "        x = F.relu(self.norm2(self.conv3(x)))\n",
    "        x = F.relu(self.norm3(self.conv4(x)))\n",
    "        x = self.conv_final(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924ee63-f789-4a2d-bb1b-4492fd9d212a",
   "metadata": {},
   "source": [
    "### Defines a residual layer to model the network architecture of the original CycleGAN paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdefb56a-2ac3-4188-9b07-5c242a90b6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual block class\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, conv_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # define two convolutional layers + batch normalization that will act as our residual function, F(x)\n",
    "        # layers should have the same shape input as output; I suggest a kernel_size of 3\n",
    "        self.conv1 = nn.Conv2d(conv_dim, conv_dim, 3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(conv_dim, conv_dim, 3, stride=1, padding=1)\n",
    "        self.norm = nn.BatchNorm2d(conv_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # apply a ReLu activation the outputs of the first layer\n",
    "        # return a summed output, x + resnet_block(x)\n",
    "        residual = x\n",
    "        x = self.norm(self.conv2(F.relu(self.conv1(x))))\n",
    "        x += residual\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd22f26-9511-4f00-b0bc-33ec507591da",
   "metadata": {},
   "source": [
    "### Creates generator using the residual blocks from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd62d2a-565f-4069-9090-97a5ad22c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_dim=64, n_res_blocks=6):\n",
    "        super(CycleGenerator, self).__init__()        \n",
    "        # 1. Define the encoder part of the generator\n",
    "        self.conv1 = nn.Conv2d(3, conv_dim, 5, stride=2, padding=2)\n",
    "        self.norm1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(conv_dim, conv_dim*2, 3, stride=2, padding=1)\n",
    "        self.norm2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(conv_dim*2, conv_dim*4, 3, stride=2, padding=1)\n",
    "        self.norm3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # 2. Define the resnet part of the generator\n",
    "        res_layers = []\n",
    "        for i in range(n_res_blocks):\n",
    "            res_layers.append(ResidualBlock(conv_dim*4))\n",
    "        self.residual = nn.Sequential(*res_layers)\n",
    "        \n",
    "        # 3. Define the decoder part of the generator\n",
    "        self.trans1 = nn.ConvTranspose2d(conv_dim*4, conv_dim*2, 4,\n",
    "                                        stride=2, padding=1, bias=False)\n",
    "        self.norm4 = nn.BatchNorm2d(conv_dim*2)\n",
    "        self.trans2 = nn.ConvTranspose2d(conv_dim*2, conv_dim, 4,\n",
    "                                        stride=2, padding=1, bias=False)\n",
    "        self.norm5 = nn.BatchNorm2d(conv_dim)\n",
    "        self.trans3 = nn.ConvTranspose2d(conv_dim, 3, 4,\n",
    "                                        stride=2, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define feedforward behavior, applying activations as necessary\n",
    "        x = F.relu(self.norm1(self.conv1(x)))\n",
    "        x = F.relu(self.norm2(self.conv2(x)))\n",
    "        x = F.relu(self.norm3(self.conv3(x)))\n",
    "        \n",
    "        x = self.residual(x)\n",
    "        \n",
    "        x = F.relu(self.norm4(self.trans1(x)))\n",
    "        x = F.relu(self.norm5(self.trans2(x)))\n",
    "        x = F.tanh(self.trans3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3201a2-e932-40e7-ba7e-ab8537083cd7",
   "metadata": {},
   "source": [
    "### Creates the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640ba9a-a5c8-45f9-9217-d067f89854dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(g_conv_dim=64, d_conv_dim=64, n_res_blocks=6):\n",
    "    # Instantiate generators\n",
    "    G_XtoY = CycleGenerator(conv_dim=g_conv_dim, n_res_blocks=n_res_blocks)\n",
    "    G_YtoX = CycleGenerator(conv_dim=g_conv_dim, n_res_blocks=n_res_blocks)\n",
    "    # Instantiate discriminators\n",
    "    D_X = Discriminator(conv_dim=d_conv_dim)\n",
    "    D_Y = Discriminator(conv_dim=d_conv_dim)\n",
    "\n",
    "    # move models to GPU, if available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        G_XtoY.to(device)\n",
    "        G_YtoX.to(device)\n",
    "        D_X.to(device)\n",
    "        D_Y.to(device)\n",
    "        print('Models moved to GPU')\n",
    "    else:\n",
    "        print('Models stay on CPU')\n",
    "\n",
    "    return G_XtoY, G_YtoX, D_X, D_Y\n",
    "\n",
    "# call the function to get models\n",
    "G_XtoY, G_YtoX, D_X, D_Y = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba937bfc-f4c2-4776-9321-c8dadc425967",
   "metadata": {},
   "source": [
    "### Defines loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75285af1-cf48-4718-a348-9db8ad7392a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions\n",
    "def real_mse_loss(D_out):\n",
    "    # how close is the produced output from being \"real\"?\n",
    "    loss = torch.mean((D_out-1)**2)\n",
    "    return loss\n",
    "\n",
    "def fake_mse_loss(D_out):\n",
    "    # how close is the produced output from being \"false\"?\n",
    "    loss = torch.mean(D_out**2)\n",
    "    return loss\n",
    "\n",
    "def cycle_consistency_loss(real_im, reconstructed_im, lambda_weight=10):\n",
    "    # calculate reconstruction loss \n",
    "    # return weighted loss\n",
    "    loss = lambda_weight * torch.mean(torch.abs(real_im-reconstructed_im))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba14f0b6-cc5d-4a35-a09e-cca273d2206e",
   "metadata": {},
   "source": [
    "### Defines optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21467c13-0465-49bd-81f0-ea721b5b2780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# hyperparams for Adam optimizers\n",
    "# got values from orinigal CycleGAN paper\n",
    "lr= 0.0002\n",
    "beta1= 0.5\n",
    "beta2= 0.999\n",
    "\n",
    "g_params = list(G_XtoY.parameters()) + list(G_YtoX.parameters())  # Get generator parameters\n",
    "\n",
    "# Create optimizers for the generators and discriminators\n",
    "g_optimizer = optim.Adam(g_params, lr, [beta1, beta2])\n",
    "d_x_optimizer = optim.Adam(D_X.parameters(), lr, [beta1, beta2])\n",
    "d_y_optimizer = optim.Adam(D_Y.parameters(), lr, [beta1, beta2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df2afe4-b6b6-4753-a869-df9c1a2fc579",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6596d-832f-4c50-b8d0-f19fc0dcf1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import save code. This was provided by Udacity\n",
    "from helpers import save_samples, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459ae9ae-3906-4cb6-9319-fe626d358a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "def training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, \n",
    "                  n_epochs=1000):\n",
    "    # keep track of losses over time\n",
    "    losses = []\n",
    "    print_every=10\n",
    "\n",
    "    test_iter_X = iter(test_dataloader_X)\n",
    "    test_iter_Y = iter(test_dataloader_Y)\n",
    "\n",
    "    # Get some fixed data from domains X and Y for sampling. These are images that are held\n",
    "    # constant throughout training, that allow us to inspect the model's performance.\n",
    "    fixed_X = test_iter_X.next()[0]\n",
    "    fixed_Y = test_iter_Y.next()[0]\n",
    "    fixed_X = scale(fixed_X) # make sure to scale to a range -1 to 1\n",
    "    fixed_Y = scale(fixed_Y)\n",
    "\n",
    "    # batches per epoch\n",
    "    iter_X = iter(dataloader_X)\n",
    "    iter_Y = iter(dataloader_Y)\n",
    "    batches_per_epoch = min(len(iter_X), len(iter_Y))\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # Reset iterators for each epoch\n",
    "        if epoch % batches_per_epoch == 0:\n",
    "            iter_X = iter(dataloader_X)\n",
    "            iter_Y = iter(dataloader_Y)\n",
    "\n",
    "        images_X, _ = iter_X.next()\n",
    "        images_X = scale(images_X) # make sure to scale to a range -1 to 1\n",
    "        images_Y, _ = iter_Y.next()\n",
    "        images_Y = scale(images_Y)\n",
    "\n",
    "        # move images to GPU if available (otherwise stay on CPU)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        images_X = images_X.to(device)\n",
    "        images_Y = images_Y.to(device)\n",
    "\n",
    "\n",
    "        # ============================================\n",
    "        #            TRAIN THE DISCRIMINATORS\n",
    "        # ============================================\n",
    "\n",
    "        ##  D_X\n",
    "        # Compute the discriminator losses on real images\n",
    "        d_out_real = D_X(images_X)\n",
    "        loss_real = real_mse_loss(d_out_real)\n",
    "        \n",
    "        # Generate fake images that look like domain X based on real images in domain Y\n",
    "        fake_im_X = G_YtoX(images_Y)\n",
    "        \n",
    "        # Compute the fake loss for D_X\n",
    "        d_out_fake = D_X(fake_im_X)\n",
    "        loss_fake = fake_mse_loss(d_out_fake)\n",
    "        \n",
    "        # Compute the total loss and perform backprop\n",
    "        d_x_loss = loss_real + loss_fake\n",
    "\n",
    "        d_x_optimizer.zero_grad()\n",
    "        d_x_loss.backward()\n",
    "        d_x_optimizer.step()\n",
    "        \n",
    "        ##  D_Y\n",
    "        d_out_real = D_Y(images_Y)\n",
    "        loss_real = real_mse_loss(d_out_real)\n",
    "        \n",
    "        fake_im_Y = G_XtoY(images_X)\n",
    "        d_out_fake = D_Y(fake_im_Y)\n",
    "        loss_fake = fake_mse_loss(d_out_fake)\n",
    "        \n",
    "        d_y_loss = loss_real + loss_fake\n",
    "        \n",
    "        d_y_optimizer.zero_grad()\n",
    "        d_y_loss.backward()\n",
    "        d_y_optimizer.step()\n",
    "\n",
    "        \n",
    "        \n",
    "        # =========================================\n",
    "        #            TRAIN THE GENERATORS\n",
    "        # =========================================\n",
    "\n",
    "        \n",
    "        # Generate fake images that look like domain X based on real images in domain Y\n",
    "        gen_YtoX = G_YtoX(images_Y)\n",
    "        # Compute the generator loss based on domain X\n",
    "        out_x = D_X(gen_YtoX)\n",
    "        d_x_genloss = real_mse_loss(out_x)\n",
    "        # Create a reconstructed y\n",
    "        recon_y = G_XtoY(gen_YtoX)\n",
    "        # Compute the cycle consistency loss (the reconstruction loss)\n",
    "        recon_y_loss = cycle_consistency_loss(images_Y, recon_y)\n",
    "\n",
    "        ## generate fake Y images and reconstructed X images\n",
    "        gen_XtoY = G_XtoY(images_X)\n",
    "        out_y = D_Y(gen_XtoY)\n",
    "        d_y_genloss = real_mse_loss(out_y)\n",
    "        recon_x = G_YtoX(gen_XtoY)\n",
    "        recon_x_loss = cycle_consistency_loss(images_X, recon_x)\n",
    "        \n",
    "        # Add up all generator and reconstructed losses and perform backprop\n",
    "        g_total_loss = d_x_genloss + recon_y_loss + d_y_genloss + recon_x_loss\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        g_total_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        # Print the log info\n",
    "        if epoch % print_every == 0:\n",
    "            # append real and fake discriminator losses and the generator loss\n",
    "            losses.append((d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\n",
    "            print('Epoch [{:5d}/{:5d}] | d_X_loss: {:6.4f} | d_Y_loss: {:6.4f} | g_total_loss: {:6.4f}'.format(\n",
    "                    epoch, n_epochs, d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\n",
    "\n",
    "            \n",
    "        sample_every=100\n",
    "        # Save the generated samples\n",
    "        if epoch % sample_every == 0:\n",
    "            G_YtoX.eval() # set generators to eval mode for sample generation\n",
    "            G_XtoY.eval()\n",
    "            save_samples(epoch, fixed_Y, fixed_X, G_YtoX, G_XtoY, batch_size=16)\n",
    "            G_YtoX.train()\n",
    "            G_XtoY.train()\n",
    "\n",
    "        #uncomment these lines, if you want to save your model\n",
    "        checkpoint_every=1000\n",
    "        # Save the model parameters\n",
    "        if epoch % checkpoint_every == 0:\n",
    "            checkpoint(epoch, G_XtoY, G_YtoX, D_X, D_Y)\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fe52a-39f9-4c2b-a960-7300703051b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 6000\n",
    "\n",
    "losses = training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634eec03-2d96-4b5f-aa0c-1e318903a936",
   "metadata": {},
   "source": [
    "### A function to view images after a certain number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0be3d4-e77a-4093-ba23-40f7419ce748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "# helper visualization code\n",
    "def view_samples(iteration, sample_dir='samples_cyclegan'):\n",
    "    \n",
    "    # samples are named by iteration\n",
    "    path_XtoY = os.path.join(sample_dir, 'sample-{:06d}-X-Y.png'.format(iteration))\n",
    "    path_YtoX = os.path.join(sample_dir, 'sample-{:06d}-Y-X.png'.format(iteration))\n",
    "    \n",
    "    # read in those samples\n",
    "    try: \n",
    "        x2y = mpimg.imread(path_XtoY)\n",
    "        y2x = mpimg.imread(path_YtoX)\n",
    "    except:\n",
    "        print('Invalid number of iterations.')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(18,20), nrows=2, ncols=1, sharey=True, sharex=True)\n",
    "    ax1.imshow(x2y)\n",
    "    ax1.set_title('X to Y')\n",
    "    ax2.imshow(y2x)\n",
    "    ax2.set_title('Y to X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511d3e7-78b1-407b-97ff-453bf6c184c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view samples at iteration 100\n",
    "view_samples(100, 'samples_cyclegan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6b9de-f3b6-4f0e-ac16-128ef9dda487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view samples at iteration 1000\n",
    "view_samples(1000, 'samples_cyclegan')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
